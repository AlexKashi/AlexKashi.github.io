<Doctype html>
<html lang = "en">
<head>
	<meta charset="utf-8">
	<title> AlexKashi - Resume </title>
	<link rel="stylesheet" type = "text/css" href= "style.css">
</head>
<body  bgcolor = "#6d7888">
	<ul class = "nav">
		<li style="float:left;  font-style: 16px; font-weight: bold;"><a href="index.html">Alex Kashi</a></li>
		<li><a href="index.html">Home</a></li>
        <li><a class = "active" href="cs294.html">CS 294</a></li>
		<li><a href="about.html">About</a></li>
		<li><a href="resume.html">Resume</a></li>
		<li><a href="contact.html">Contact</a></li>
	</ul>
    <div class="cs294-content">
    	<h1 style = "text-align: center;"> CS294 Deep Reinforcment Solutions </h1>
    	<h2> Purpose and Introduciton </h2>
    	<p style = "text-indent: 50px;">Hello and Welcome, I will be posting up my solutions semi-formally for the Berkeley course:
    	<a href="http://rll.berkeley.edu/deeprlcourse/"> CS294-112 Deep Reinforcment Learning, Fall 2017</a>.
		As for my background I graduated from Berkeley with a degree in Electrical Engineering and Computer Sciences in 2017. 
    	I always wanted to take this course, although it was not offered while I was qualified. So I decided to take the class in my free time by following along online, 
    	like you can! </p>
    	<p style = "text-indent: 50px;"> I have already got the first couple of homeworks completed, it is just a matter of cleaning up the code, and writing it up here. I thought I would share my solutions to help others, and to make up for the fact I am not physically in the class forcing me to fully understand the material and not skip over the fine, important, details. </p>
    	<h2> <a href="http://rll.berkeley.edu/deeprlcourse/f17docs/hw1fall2017.pdf">  Deep RL Assignment 1: Imitation Learning </a> </h2>
    	<h3  style = "text-indent: 25px;">Asignment Preview </h3>
    	<p style = "text-indent: 50px;"> In this assignment we get acquainted with Tensorflow, OpenAI Gym, MuJoCo, and implement some basic learning algorithms. First we try behavioral cloning and notice that it works well in some simple cases, but seems to diverge when things get complex. This is because of the compounding error fomononan that occurs when you have no global feedback (similar to dead reckoning). As time goes on your observations get farther and farther away from the observations you trained on, meaning your classifier is less equipt to classify them correctly. When your training data is not from the same policy as your test data, we can say nothing about the test performance, even if the training performance is perfect. To remedy this issue we implement another algorithm DAgger, which allows us to get on policy data by having an expert produce actions for what the agent should have done, given the current observation in the test set. Then aggregate these observation, action pairs into the training data and retrain on the new dataset, and iterate. DAgger is useful in some cases but the fact that an expert (usually human) has to reclassify what the agent should have done, can be time consuming and expensive in the best case, and downright impossible in the worst. 
    </div>
</body>
</html>
